{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4CV Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oziris/DL4CV/blob/master/DL4CV_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mha5lH5uRWES",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Networks and Deep Learning is a free online book. The book will teach you about:\n",
        "\n",
        "*   Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data\n",
        "*   Deep learning, a powerful set of techniques for learning in neural networks\n",
        "\n",
        "Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.\n",
        "For more details about the approach taken in the book, see here. Or you can jump directly to Chapter 1 and get started.\n",
        "\n",
        "Book: http://neuralnetworksanddeeplearning.com/ \n",
        "\n",
        "Code: https://github.com/chengfx/neural-networks-and-deep-learning-for-python3"
      ]
    },
    {
      "metadata": {
        "id": "mlS5jYiYRvuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import random\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o0Fr7h0tR1vR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\tdef __init__(self,sizes): #the list sizes contains the number of neurons in the respective layers.\n",
        "\t\tself.num_layers = len(sizes)  #the number of the layers in Network\n",
        "\t\tself.sizes = sizes\n",
        "\t\tself.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
        "\t\tself.weights = [np.random.randn(y,x) \n",
        "\t\t\t\t\t\tfor x,y in zip(sizes[:-1],sizes[1:])]\n",
        "\n",
        "\tdef feedforward(self,a):\n",
        "\t\t\"\"\"Return the output of the network if \"a\" is input\"\"\"\n",
        "\t\tfor b,w in zip(self.biases,self.weights):\n",
        "\t\t\ta = sigmoid(np.dot(w,a) + b)\t\t\n",
        "\t\treturn a\n",
        "\t\t\n",
        "\tdef SGD(self, training_data, epochs, mini_batch_size,eta,\n",
        "\t        test_data = None):\n",
        "\t\t\"\"\"\n",
        "\t\tTrain the neural network using mini-batch stochastic gradient descent.\n",
        "\t\tThe \"training_data\" is a list of tuples \"(x,y)\" representing the training inputs\n",
        "\t\tand the desired output. The other non-optional parameters are self-explanatory.\n",
        "\t\tIf \"test_data\" is provided then the network will be evaluated against the test \n",
        "\t\tdata after each epoch, and partial progress printed out. This is useful for tracking\n",
        "\t\tprocess, but slows things down substantially.\n",
        "\t\t\"\"\"\n",
        "\t\tif test_data is not None: \n",
        "\t\t\tn_test = len(test_data)\n",
        "\t\tn = len(training_data)\n",
        "\t\tfor j in range(epochs):\n",
        "\t\t\trandom.shuffle(training_data)        #rearrange the training_data randomly\n",
        "\t\t\tmini_batches = [ training_data[k:k + mini_batch_size]\n",
        "\t\t\t                               for k in range(0, n, mini_batch_size)]\n",
        "\t\t\tfor mini_batch in mini_batches:\n",
        "\t\t\t\tself.update_mini_batch(mini_batch,eta)\n",
        "\t\t\tif test_data:\n",
        "\t\t\t\tprint(\"Epoch {0}: {1} / {2}\".format(j,self.evaluate(test_data),n_test))\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"Epoch {0} complete\".format(j))\n",
        "\t\t\t\n",
        "\tdef update_mini_batch(self,mini_batch,eta):\n",
        "\t\t\"\"\"\n",
        "\t\tUpdate the network's weights and biases by applying gradient descent using backpropagation\n",
        "\t\tto a single mini batch. The \"mini_batch\" is a list of tuples \"(x,y)\" and \"eta\" is the learning\n",
        "\t\trate.\n",
        "\t\t\"\"\"\n",
        "\t\tnabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\t\tnabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\t\tfor x, y in mini_batch:\n",
        "\t\t\tdelta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "\t\t\tnabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "\t\t\tnabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\t\tself.weights = [w-(eta/len(mini_batch))*nw \n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "\t\tself.biases = [b-(eta/len(mini_batch))*nb \n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\t\n",
        "\tdef backprop(self, x, y):\n",
        "\t\t\"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
        "        gradient for the cost function C_x.  ``nabla_b`` and\n",
        "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
        "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
        "\t\tnabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\t\tnabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "\t\tactivation = x\n",
        "\t\tactivations = [x] # list to store all the activations, layer by layer\n",
        "\t\tzs = [] # list to store all the z vectors, layer by layer\n",
        "\t\tfor b, w in zip(self.biases, self.weights):\n",
        "\t\t\tz = np.dot(w, activation)+b\n",
        "\t\t\tzs.append(z)\n",
        "\t\t\tactivation = sigmoid(z)\n",
        "\t\t\tactivations.append(activation)\n",
        "        # backward pass\n",
        "\t\tdelta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "\t\tnabla_b[-1] = delta\n",
        "\t\tnabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "\t\tfor l in range(2, self.num_layers):\n",
        "\t\t\tz = zs[-l]\n",
        "\t\t\tsp = sigmoid_prime(z)\n",
        "\t\t\tdelta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "\t\t\tnabla_b[-l] = delta\n",
        "\t\t\tnabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\t\treturn (nabla_b, nabla_w)\n",
        "\n",
        "\tdef evaluate(self, test_data):\n",
        "\t\t\"\"\"Return the number of test inputs for which the neural\n",
        "        network outputs the correct result. Note that the neural\n",
        "        network's output is assumed to be the index of whichever\n",
        "        neuron in the final layer has the highest activation.\"\"\"\n",
        "\t\ttest_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "\t\treturn sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "\tdef cost_derivative(self, output_activations, y):\n",
        "\t\t\"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "        \\partial a for the output activations.\"\"\"\n",
        "\t\treturn (output_activations-y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RSfA8qrR5xW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4D4hSsyIZc0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "599582c0-41db-4773-9d41-922871fdd54b"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images/255.0\n",
        "\n",
        "print(training_images.shape)\n",
        "print(training_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQ-tkCHMjWgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "05dbf06a-748a-4bfe-ef5f-61af15b66727"
      },
      "cell_type": "code",
      "source": [
        "training_images_aux = [np.reshape(x, (784, 1)) for x in training_images]\n",
        "test_images_aux = [np.reshape(x, (784, 1)) for x in test_images]\n",
        "training_labels_aux = [np.reshape(to_categorical(y, num_classes=10), (10,1)) for y in training_labels]\n",
        "\n",
        "print(training_images_aux[0].shape)\n",
        "print(training_labels_aux[0].shape)\n",
        "print(test_images_aux[0].shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 1)\n",
            "(10, 1)\n",
            "(784, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ujHTKpACccuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3b71c5d5-1f36-4623-c085-964a4d0064e1"
      },
      "cell_type": "code",
      "source": [
        "training_data = list(zip(training_images_aux, training_labels_aux))\n",
        "test_data = list(zip(test_images_aux, test_labels))\n",
        "\n",
        "net = Network([784, 30, 10])\n",
        "\n",
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 9111 / 10000\n",
            "Epoch 1: 9229 / 10000\n",
            "Epoch 2: 9329 / 10000\n",
            "Epoch 3: 9352 / 10000\n",
            "Epoch 4: 9409 / 10000\n",
            "Epoch 5: 9408 / 10000\n",
            "Epoch 6: 9389 / 10000\n",
            "Epoch 7: 9403 / 10000\n",
            "Epoch 8: 9455 / 10000\n",
            "Epoch 9: 9422 / 10000\n",
            "Epoch 10: 9465 / 10000\n",
            "Epoch 11: 9450 / 10000\n",
            "Epoch 12: 9469 / 10000\n",
            "Epoch 13: 9457 / 10000\n",
            "Epoch 14: 9479 / 10000\n",
            "Epoch 15: 9478 / 10000\n",
            "Epoch 16: 9469 / 10000\n",
            "Epoch 17: 9485 / 10000\n",
            "Epoch 18: 9477 / 10000\n",
            "Epoch 19: 9459 / 10000\n",
            "Epoch 20: 9463 / 10000\n",
            "Epoch 21: 9464 / 10000\n",
            "Epoch 22: 9466 / 10000\n",
            "Epoch 23: 9497 / 10000\n",
            "Epoch 24: 9456 / 10000\n",
            "Epoch 25: 9492 / 10000\n",
            "Epoch 26: 9467 / 10000\n",
            "Epoch 27: 9460 / 10000\n",
            "Epoch 28: 9478 / 10000\n",
            "Epoch 29: 9492 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RX0L3N6zt7MO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "612b4183-ef98-4219-fdb3-ecb66a8081d2"
      },
      "cell_type": "code",
      "source": [
        "idx = 88\n",
        "print(\"Label:\", test_labels[idx])\n",
        "net.feedforward(test_images_aux[idx])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.10741863e-06],\n",
              "       [6.85184195e-08],\n",
              "       [5.32040531e-07],\n",
              "       [5.71567368e-10],\n",
              "       [1.83839676e-05],\n",
              "       [5.05825429e-11],\n",
              "       [9.99981648e-01],\n",
              "       [6.50114365e-09],\n",
              "       [1.01372711e-06],\n",
              "       [1.15202397e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}